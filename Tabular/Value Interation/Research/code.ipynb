{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533b7b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gymnasium[toy_text] numpy tensorboard tensorboardX\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "ARROW_MAP = {0:\"←\", 1:\"↓\", 2:\"→\", 3:\"↑\"}  # Gymnasium: 0=L,1=D,2=R,3=U\n",
    "\n",
    "# -------------------- Seeding Utils --------------------\n",
    "def set_global_seed(seed: int):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def make_env(desc=None, map_name=None, is_slippery=False, seed: int | None = None):\n",
    "    # Hinweis: desc und map_name nicht gleichzeitig setzen\n",
    "    kwargs = {\"is_slippery\": is_slippery}\n",
    "    if desc is not None:\n",
    "        kwargs[\"desc\"] = desc\n",
    "    if map_name is not None:\n",
    "        kwargs[\"map_name\"] = map_name\n",
    "\n",
    "    env = gym.make(\"FrozenLake-v1\", **kwargs)\n",
    "    # WICHTIG: seed beim reset setzen, zusätzlich Spaces seeden\n",
    "    if seed is not None:\n",
    "        env.reset(seed=seed)\n",
    "        try:\n",
    "            env.action_space.seed(seed)\n",
    "            env.observation_space.seed(seed)\n",
    "        except Exception:\n",
    "            pass\n",
    "    else:\n",
    "        env.reset()\n",
    "    return env\n",
    "\n",
    "# -------------------- Algo + Eval --------------------\n",
    "def run_episode(env, policy, reset_seed: int | None = None, max_steps=10_000):\n",
    "    if reset_seed is None:\n",
    "        obs, _ = env.reset()\n",
    "    else:\n",
    "        obs, _ = env.reset(seed=reset_seed)\n",
    "\n",
    "    total_r, steps, done = 0.0, 0, False\n",
    "    while not done and steps < max_steps:\n",
    "        a = int(policy[obs])\n",
    "        obs, r, done, truncated, _ = env.step(a)\n",
    "        total_r += r\n",
    "        steps += 1\n",
    "        if truncated:\n",
    "            break\n",
    "    return total_r, steps, done\n",
    "\n",
    "def evaluate_policy(env, policy, episodes=50, base_seed: int | None = None):\n",
    "    # deterministische Folge von Episoden-Seeds\n",
    "    rng = np.random.default_rng(base_seed) if base_seed is not None else None\n",
    "    wins, total_return, total_steps = 0, 0.0, 0\n",
    "    for _ in range(episodes):\n",
    "        ep_seed = int(rng.integers(0, 2**31-1)) if rng is not None else None\n",
    "        ret, steps, done = run_episode(env, policy, reset_seed=ep_seed)\n",
    "        total_return += ret\n",
    "        total_steps += steps\n",
    "        wins += int(done and ret > 0)\n",
    "    return (total_return / episodes), (wins / episodes), (total_steps / episodes)\n",
    "\n",
    "def value_iteration(env, gamma=0.99, theta=1e-8, max_iters=10_000,\n",
    "                    writer: SummaryWriter | None = None,\n",
    "                    eval_every: int = 0, eval_episodes: int = 50,\n",
    "                    eval_seed: int | None = None,\n",
    "                    verbose=False):\n",
    "    P = env.unwrapped.P\n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n\n",
    "    V = np.zeros(nS, dtype=np.float64)\n",
    "\n",
    "    for it in range(max_iters):\n",
    "        delta = 0.0\n",
    "        for s in range(nS):\n",
    "            v_old = V[s]\n",
    "            best = -np.inf\n",
    "            for a in range(nA):\n",
    "                q = 0.0\n",
    "                for prob, s_next, r, done in P[s][a]:\n",
    "                    q += prob * (r + (0.0 if done else gamma * V[s_next]))\n",
    "                if q > best:\n",
    "                    best = q\n",
    "            V[s] = best\n",
    "            delta = max(delta, abs(v_old - V[s]))\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\"train/delta\", float(delta), it)\n",
    "            writer.add_scalar(\"train/mean_V\", float(np.mean(V)), it)\n",
    "            writer.add_scalar(\"train/max_V\", float(np.max(V)), it)\n",
    "\n",
    "        if eval_every and (it % eval_every == 0 or it == max_iters - 1):\n",
    "            policy = extract_policy(env, V, gamma=gamma)\n",
    "            avg_ret, success, avg_steps = evaluate_policy(env, policy, episodes=eval_episodes, base_seed=eval_seed)\n",
    "            if writer is not None:\n",
    "                writer.add_scalar(\"eval/avg_return\", float(avg_ret), it)\n",
    "                writer.add_scalar(\"eval/success_rate\", float(success), it)\n",
    "                writer.add_scalar(\"eval/avg_steps\", float(avg_steps), it)\n",
    "\n",
    "        if verbose and it % 50 == 0:\n",
    "            print(f\"Iter {it:4d}  delta={delta:.3e}  meanV={np.mean(V):.4f}\")\n",
    "        if delta < theta:\n",
    "            if verbose:\n",
    "                print(f\"Konvergiert nach {it} Iterationen (delta={delta:.3e}).\")\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def extract_policy(env, V, gamma=0.99):\n",
    "    P = env.unwrapped.P\n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n\n",
    "    policy = np.zeros(nS, dtype=np.int64)\n",
    "\n",
    "    for s in range(nS):\n",
    "        q_values = np.zeros(nA, dtype=np.float64)\n",
    "        for a in range(nA):\n",
    "            for prob, s_next, r, done in P[s][a]:\n",
    "                q_values[a] += prob * (r + (0.0 if done else gamma * V[s_next]))\n",
    "        policy[s] = int(np.argmax(q_values))\n",
    "    return policy\n",
    "\n",
    "def render_policy_grid(env, policy):\n",
    "    n = int(np.sqrt(env.observation_space.n))\n",
    "    desc = env.unwrapped.desc.astype(str).copy()\n",
    "    for s, a in enumerate(policy):\n",
    "        r, c = divmod(s, n)\n",
    "        if desc[r, c] in (\"H\", \"G\", \"S\"):\n",
    "            continue\n",
    "        desc[r, c] = ARROW_MAP[a]\n",
    "    print(\"\\nPolicy (Pfeile), S=Start, G=Goal, H=Hole:\")\n",
    "    for r in range(n):\n",
    "        print(\" \".join(desc[r]))\n",
    "\n",
    "# -------------------- Main --------------------\n",
    "def main():\n",
    "    SEED = 12345  # <— ändere hier für reproduzierbare Runs\n",
    "    set_global_seed(SEED)\n",
    "\n",
    "    # Env wählen\n",
    "    custom_map = [\"SFFF\",\"FHFH\",\"FFFH\",\"HFFG\"]\n",
    "    env = make_env(desc=custom_map, is_slippery=False, seed=SEED)\n",
    "    # Alternativen:\n",
    "    # env = make_env(map_name=\"4x4\", is_slippery=True, seed=SEED)\n",
    "    # env = make_env(map_name=\"4x4\", is_slippery=False, seed=SEED)\n",
    "\n",
    "    gamma = 0.99\n",
    "    theta = 1e-8\n",
    "\n",
    "    run_name = f\"VI_FrozenLake_seed{SEED}_{int(time.time())}\"\n",
    "    writer = SummaryWriter(logdir=f\"runs/{run_name}\")\n",
    "    writer.add_text(\"meta/run\", run_name, 0)\n",
    "    writer.add_text(\"meta/seed\", str(SEED), 0)\n",
    "    writer.add_text(\"meta/env\", f\"is_slippery={env.unwrapped.is_slippery}, map={custom_map}\", 0)\n",
    "    writer.add_text(\"meta/hparams\", f\"gamma={gamma}, theta={theta}\", 0)\n",
    "\n",
    "    V = value_iteration(\n",
    "        env,\n",
    "        gamma=gamma,\n",
    "        theta=theta,\n",
    "        max_iters=10_000,\n",
    "        writer=writer,\n",
    "        eval_every=5,          # alle 5 Iterationen evaluieren\n",
    "        eval_episodes=50,\n",
    "        eval_seed=SEED,        # sorgt für reproduzierbare Eval-Episoden\n",
    "        verbose=True\n",
    "    )\n",
    "    policy = extract_policy(env, V, gamma=gamma)\n",
    "\n",
    "    # finale Evaluation mit größerem Sample\n",
    "    avg_ret, success, avg_steps = evaluate_policy(env, policy, episodes=200, base_seed=SEED)\n",
    "    writer.add_scalar(\"final/avg_return\", float(avg_ret))\n",
    "    writer.add_scalar(\"final/success_rate\", float(success))\n",
    "    writer.add_scalar(\"final/avg_steps\", float(avg_steps))\n",
    "    writer.close()\n",
    "\n",
    "    n = int(np.sqrt(env.observation_space.n))\n",
    "    print(\"\\nValue-Funktion (reshaped):\")\n",
    "    print(V.reshape(n, n))\n",
    "    render_policy_grid(env, policy)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c4f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "python main.py\n",
    "tensorboard --logdir runs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
