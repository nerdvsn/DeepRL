{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5c6d1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # sollte True sein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afcf0b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Ø Reward = 12.98\n",
      "Iteration 2: Ø Reward = 15.92\n",
      "Iteration 3: Ø Reward = 26.77\n",
      "Iteration 4: Ø Reward = 30.53\n",
      "Iteration 5: Ø Reward = 37.63\n",
      "Iteration 6: Ø Reward = 40.87\n",
      "Iteration 7: Ø Reward = 43.32\n",
      "Iteration 8: Ø Reward = 48.06\n",
      "Iteration 9: Ø Reward = 59.81\n",
      "Iteration 10: Ø Reward = 50.81\n",
      "Iteration 11: Ø Reward = 59.40\n",
      "Iteration 12: Ø Reward = 57.00\n",
      "Iteration 13: Ø Reward = 62.67\n",
      "Iteration 14: Ø Reward = 57.97\n",
      "Iteration 15: Ø Reward = 62.27\n",
      "Iteration 16: Ø Reward = 64.21\n",
      "Iteration 17: Ø Reward = 70.72\n",
      "Iteration 18: Ø Reward = 75.98\n",
      "Iteration 19: Ø Reward = 78.98\n",
      "Iteration 20: Ø Reward = 76.78\n",
      "Iteration 21: Ø Reward = 70.79\n",
      "Iteration 22: Ø Reward = 81.61\n",
      "Iteration 23: Ø Reward = 77.35\n",
      "Iteration 24: Ø Reward = 81.58\n",
      "Iteration 25: Ø Reward = 90.07\n",
      "Iteration 26: Ø Reward = 88.70\n",
      "Iteration 27: Ø Reward = 89.69\n",
      "Iteration 28: Ø Reward = 89.79\n",
      "Iteration 29: Ø Reward = 93.07\n",
      "Iteration 30: Ø Reward = 92.19\n",
      "    Iteration  Average Reward\n",
      "25         26           88.70\n",
      "26         27           89.69\n",
      "27         28           89.79\n",
      "28         29           93.07\n",
      "29         30           92.19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('cem_nn_cartpole_plot.png', 135.0, 'cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# CUDA prüfen\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Umgebung\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Hyperparameter\n",
    "N = 100              # Anzahl Policy-Netzwerke pro Runde\n",
    "elite_frac = 0.2     # Top X % behalten\n",
    "n_elite = int(N * elite_frac)\n",
    "n_iterations = 30\n",
    "max_episode_length = 500\n",
    "hidden_size = 32     # Anzahl Neuronen in versteckter Schicht\n",
    "lr = 1e-2\n",
    "\n",
    "# Policy-Netzwerk\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Policy bewerten\n",
    "def evaluate_policy(policy):\n",
    "    policy.eval()\n",
    "    total_reward = 0\n",
    "    obs = env.reset()\n",
    "    if isinstance(obs, tuple): obs = obs[0]\n",
    "    for _ in range(max_episode_length):\n",
    "        obs_tensor = torch.FloatTensor(obs).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = policy(obs_tensor)\n",
    "        action = torch.argmax(logits).item()\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "# Initiale Population von Netzwerken\n",
    "def sample_population(base_policy, std=0.1):\n",
    "    population = []\n",
    "    for _ in range(N):\n",
    "        new_policy = PolicyNet().to(device)\n",
    "        new_policy.load_state_dict(base_policy.state_dict())\n",
    "        with torch.no_grad():\n",
    "            for p in new_policy.parameters():\n",
    "                p.add_(torch.randn_like(p) * std)\n",
    "        population.append(new_policy)\n",
    "    return population\n",
    "\n",
    "# Trainingsschleife\n",
    "base_policy = PolicyNet().to(device)\n",
    "mean_rewards = []\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    population = sample_population(base_policy, std=0.1)\n",
    "    rewards = [evaluate_policy(p) for p in population]\n",
    "\n",
    "    # Top-Performer auswählen\n",
    "    elite_indices = np.argsort(rewards)[-n_elite:]\n",
    "    elite_policies = [population[i] for i in elite_indices]\n",
    "\n",
    "    # Neue Basis-Policy trainieren (Supervised Learning auf Top-Population)\n",
    "    optimizer = optim.Adam(base_policy.parameters(), lr=lr)\n",
    "\n",
    "    # Sammle Beobachtungen + Aktionen der Elite-Population\n",
    "    obs_list, act_list = [], []\n",
    "    for policy in elite_policies:\n",
    "        obs = env.reset()\n",
    "        if isinstance(obs, tuple): obs = obs[0]\n",
    "        for _ in range(max_episode_length):\n",
    "            obs_tensor = torch.FloatTensor(obs).to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = policy(obs_tensor)\n",
    "            action = torch.argmax(logits).item()\n",
    "            obs_list.append(obs)\n",
    "            act_list.append(action)\n",
    "            obs, _, terminated, truncated, _ = env.step(action)\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "    # Training auf Elite-Beobachtungen\n",
    "    base_policy.train()\n",
    "    obs_batch = torch.FloatTensor(obs_list).to(device)\n",
    "    act_batch = torch.LongTensor(act_list).to(device)\n",
    "\n",
    "    logits = base_policy(obs_batch)\n",
    "    loss = nn.CrossEntropyLoss()(logits, act_batch)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    avg_reward = np.mean(rewards)\n",
    "    mean_rewards.append(avg_reward)\n",
    "    print(f\"Iteration {iteration+1}: Ø Reward = {avg_reward:.2f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(mean_rewards)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.title(\"Cross Entropy Method mit Neuronalen Netz (CartPole)\")\n",
    "plt.grid(True)\n",
    "plot_path = \"cem_nn_cartpole_plot.png\"\n",
    "plt.savefig(plot_path)\n",
    "plt.close()\n",
    "\n",
    "# Beste Policy testen\n",
    "final_policy = base_policy.eval()\n",
    "obs = env.reset()\n",
    "if isinstance(obs, tuple): obs = obs[0]\n",
    "total_reward = 0\n",
    "for _ in range(max_episode_length):\n",
    "    obs_tensor = torch.FloatTensor(obs).to(device)\n",
    "    with torch.no_grad():\n",
    "        action = torch.argmax(final_policy(obs_tensor)).item()\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "        \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Iteration\": list(range(1, n_iterations + 1)),\n",
    "    \"Average Reward\": mean_rewards\n",
    "})\n",
    "\n",
    "print(df.tail())  # zeigt letzte 5 Zeilen\n",
    "\n",
    "\n",
    "plot_path, total_reward, device.type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c1d9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
