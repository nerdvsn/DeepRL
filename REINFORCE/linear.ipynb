{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb8cf64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Ø Reward = 67.91\n",
      "Iteration 2: Ø Reward = 206.60\n",
      "Iteration 3: Ø Reward = 281.57\n",
      "Iteration 4: Ø Reward = 361.57\n",
      "Iteration 5: Ø Reward = 388.00\n",
      "Iteration 6: Ø Reward = 448.25\n",
      "Iteration 7: Ø Reward = 476.22\n",
      "Iteration 8: Ø Reward = 468.65\n",
      "Iteration 9: Ø Reward = 469.06\n",
      "Iteration 10: Ø Reward = 482.04\n",
      "Iteration 11: Ø Reward = 470.92\n",
      "Iteration 12: Ø Reward = 495.04\n",
      "Iteration 13: Ø Reward = 499.99\n",
      "Iteration 14: Ø Reward = 499.59\n",
      "Iteration 15: Ø Reward = 500.00\n",
      "Iteration 16: Ø Reward = 498.72\n",
      "Iteration 17: Ø Reward = 500.00\n",
      "Iteration 18: Ø Reward = 500.00\n",
      "Iteration 19: Ø Reward = 499.27\n",
      "Iteration 20: Ø Reward = 500.00\n",
      "Iteration 21: Ø Reward = 499.17\n",
      "Iteration 22: Ø Reward = 498.43\n",
      "Iteration 23: Ø Reward = 499.92\n",
      "Iteration 24: Ø Reward = 500.00\n",
      "Iteration 25: Ø Reward = 499.14\n",
      "Iteration 26: Ø Reward = 496.96\n",
      "Iteration 27: Ø Reward = 499.26\n",
      "Iteration 28: Ø Reward = 497.36\n",
      "Iteration 29: Ø Reward = 499.59\n",
      "Iteration 30: Ø Reward = 500.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('cem_cartpole_plot.png', 500.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Umgebung initialisieren\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs_size = env.observation_space.shape[0]  # 4\n",
    "n_actions = env.action_space.n  # 2 (links/rechts)\n",
    "\n",
    "# Cross-Entropy-Hyperparameter\n",
    "N = 100  # Anzahl gesampelter Policies pro Runde\n",
    "elite_frac = 0.2\n",
    "n_elite = int(N * elite_frac)\n",
    "n_iterations = 30\n",
    "max_episode_length = 500\n",
    "\n",
    "# Policy-Parameter (lineares Modell: action = argmax(W @ obs))\n",
    "mu = np.zeros((n_actions, obs_size))\n",
    "std = np.ones((n_actions, obs_size))\n",
    "\n",
    "# Zum Plotten\n",
    "mean_rewards = []\n",
    "\n",
    "# Policy ausführen\n",
    "def evaluate_policy(weights):\n",
    "    total_reward = 0\n",
    "    obs = env.reset()\n",
    "    if isinstance(obs, tuple):  # für Gym v0.26+\n",
    "        obs = obs[0]\n",
    "    for _ in range(max_episode_length):\n",
    "        logits = weights @ obs\n",
    "        action = np.argmax(logits)\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "# Trainingsschleife\n",
    "for iteration in range(n_iterations):\n",
    "    # 1. Policies generieren\n",
    "    policies = [np.random.normal(mu, std) for _ in range(N)]\n",
    "\n",
    "    # 2. Rewards evaluieren\n",
    "    rewards = [evaluate_policy(p) for p in policies]\n",
    "\n",
    "    # 3. Elite auswählen\n",
    "    elite_indices = np.argsort(rewards)[-n_elite:]\n",
    "    elite_policies = np.array([policies[i] for i in elite_indices])\n",
    "\n",
    "    # 4. Mittelwert & Streuung aktualisieren\n",
    "    mu = elite_policies.mean(axis=0)\n",
    "    std = elite_policies.std(axis=0)\n",
    "\n",
    "    # Logging\n",
    "    avg_reward = np.mean(rewards)\n",
    "    mean_rewards.append(avg_reward)\n",
    "    print(f\"Iteration {iteration+1}: Ø Reward = {avg_reward:.2f}\")\n",
    "\n",
    "# Ergebnis visualisieren\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(mean_rewards)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Durchschnittlicher Reward\")\n",
    "plt.title(\"CEM mit CartPole (lineare Policy)\")\n",
    "plt.grid(True)\n",
    "plot_path = \"cem_cartpole_plot.png\"\n",
    "plt.savefig(plot_path)\n",
    "plt.close()\n",
    "\n",
    "# Beste Policy testen\n",
    "best_policy = mu\n",
    "obs = env.reset()\n",
    "if isinstance(obs, tuple):\n",
    "    obs = obs[0]\n",
    "total_reward = 0\n",
    "for _ in range(max_episode_length):\n",
    "    action = np.argmax(best_policy @ obs)\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "\n",
    "plot_path, total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86476492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration  Average Reward\n",
      "25         26          496.96\n",
      "26         27          499.26\n",
      "27         28          497.36\n",
      "28         29          499.59\n",
      "29         30          500.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Iteration\": list(range(1, n_iterations + 1)),\n",
    "    \"Average Reward\": mean_rewards\n",
    "})\n",
    "\n",
    "print(df.tail(5))  # zeige die letzten 5 Iterationen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89b21c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
