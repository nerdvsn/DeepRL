{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aa89d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimale Wertefunktion V(s):\n",
      "V(a) = 12.1395\n",
      "V(b) = 5.8605\n",
      "\n",
      "Optimale Policy π(s):\n",
      "Im Zustand a → beste Aktion: a\n",
      "Im Zustand b → beste Aktion: a\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Zustände und Aktionen\n",
    "STATES = [\"a\", \"b\"]\n",
    "ACTIONS = [\"a\", \"b\"]\n",
    "\n",
    "# Belohnungsmatrix: REWARDS[state][next_state]\n",
    "REWARDS = {\n",
    "    \"a\": {\"a\": 0, \"b\": 7},\n",
    "    \"b\": {\"a\": -5, \"b\": 0}\n",
    "}\n",
    "\n",
    "# Übergangswahrscheinlichkeiten: TRANSITIONS[state][next_state]\n",
    "TRANSITIONS = {\n",
    "    \"a\": {\"a\": 0.1, \"b\": 0.9},\n",
    "    \"b\": {\"a\": 0.9, \"b\": 0.1}\n",
    "}\n",
    "\n",
    "# Hyperparameter\n",
    "gamma = 0.9     # Diskontierungsfaktor\n",
    "theta = 1e-6    # Schwelle für Konvergenz\n",
    "\n",
    "# Wertefunktion initialisieren\n",
    "V = {s: 0.0 for s in STATES}\n",
    "\n",
    "# Value Iteration Algorithmus\n",
    "while True:\n",
    "    delta = 0\n",
    "    for state in STATES:\n",
    "        v = V[state]\n",
    "        # Berechne erwarteten Wert für jede Aktion\n",
    "        action_values = {}\n",
    "        for action in ACTIONS:\n",
    "            expected_value = 0\n",
    "            for next_state, prob in TRANSITIONS[state].items():\n",
    "                reward = REWARDS[state][next_state]\n",
    "                expected_value += prob * (reward + gamma * V[next_state])\n",
    "            action_values[action] = expected_value\n",
    "        # Wähle die beste Aktion (maximiert erwarteten Wert)\n",
    "        V[state] = max(action_values.values())\n",
    "        delta = max(delta, abs(v - V[state]))\n",
    "    if delta < theta:\n",
    "        break\n",
    "\n",
    "# Optimale Policy extrahieren\n",
    "policy = {}\n",
    "for state in STATES:\n",
    "    best_action = None\n",
    "    best_value = float('-inf')\n",
    "    for action in ACTIONS:\n",
    "        expected_value = 0\n",
    "        for next_state, prob in TRANSITIONS[state].items():\n",
    "            reward = REWARDS[state][next_state]\n",
    "            expected_value += prob * (reward + gamma * V[next_state])\n",
    "        if expected_value > best_value:\n",
    "            best_value = expected_value\n",
    "            best_action = action\n",
    "    policy[state] = best_action\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "print(\"Optimale Wertefunktion V(s):\")\n",
    "for state in STATES:\n",
    "    print(f\"V({state}) = {V[state]:.4f}\")\n",
    "\n",
    "print(\"\\nOptimale Policy π(s):\")\n",
    "for state in STATES:\n",
    "    print(f\"Im Zustand {state} → beste Aktion: {policy[state]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad497358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
